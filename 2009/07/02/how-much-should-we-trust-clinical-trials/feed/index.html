<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd"
xmlns:rawvoice="http://www.rawvoice.com/rawvoiceRssModule/"

	>
<channel>
	<title>Comments on: How Much Should We Trust Clinical Trials?</title>
	<atom:link href="http://blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/feed/" rel="self" type="application/rss+xml" />
	<link>http://blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/</link>
	<description>Personal Science, Self-Experimentation, Scientific Method</description>
	<lastBuildDate>Wed, 17 Sep 2014 20:18:18 +0000</lastBuildDate>
		<sy:updatePeriod>hourly</sy:updatePeriod>
		<sy:updateFrequency>1</sy:updateFrequency>
	<generator>http://wordpress.org/?v=3.7.1</generator>
	<item>
		<title>By: Don</title>
		<link>http://blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/#comment-393154</link>
		<dc:creator><![CDATA[Don]]></dc:creator>
		<pubDate>Tue, 16 Feb 2010 19:47:34 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/#comment-393154</guid>
		<description><![CDATA[thanks for your posting. I loved the comments and that back and forth. 
It reminds me, in some slight way, about how clinical trials come about, and how they are used to find both negative and positive results. 
We all need to go back and forth and finalize and make our opinions clearer as we move forward. 
Thank you for an enlightening post!]]></description>
		<content:encoded><![CDATA[<p>thanks for your posting. I loved the comments and that back and forth.<br />
It reminds me, in some slight way, about how clinical trials come about, and how they are used to find both negative and positive results.<br />
We all need to go back and forth and finalize and make our opinions clearer as we move forward.<br />
Thank you for an enlightening post!</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Fri, Jul 17th &#8211; CrossFit Ireland - Great People. Great Fitness.</title>
		<link>http://blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/#comment-324941</link>
		<dc:creator><![CDATA[Fri, Jul 17th &#8211; CrossFit Ireland - Great People. Great Fitness.]]></dc:creator>
		<pubDate>Thu, 16 Jul 2009 23:02:24 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/#comment-324941</guid>
		<description><![CDATA[[...] Perfectionism - Brian Degenero How Much Should We Trust Clinical Trials - Seth Roberts [...]]]></description>
		<content:encoded><![CDATA[<p>[...] Perfectionism &#8211; Brian Degenero How Much Should We Trust Clinical Trials &#8211; Seth Roberts [...]</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Seth&#8217;s blog &#187; Blog Archive &#187; Yogurt Associated With Less Allergies</title>
		<link>http://blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/#comment-323293</link>
		<dc:creator><![CDATA[Seth&#8217;s blog &#187; Blog Archive &#187; Yogurt Associated With Less Allergies]]></dc:creator>
		<pubDate>Sat, 11 Jul 2009 16:15:02 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/#comment-323293</guid>
		<description><![CDATA[[...] Note the small sample size. Contrary to some experts, it&#8217;s a good sign. It means the differences were strong enough to be significant in a relatively small sample. [...]]]></description>
		<content:encoded><![CDATA[<p>[...] Note the small sample size. Contrary to some experts, it&#8217;s a good sign. It means the differences were strong enough to be significant in a relatively small sample. [...]</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Tom</title>
		<link>http://blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/#comment-321262</link>
		<dc:creator><![CDATA[Tom]]></dc:creator>
		<pubDate>Sun, 05 Jul 2009 02:56:42 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/#comment-321262</guid>
		<description><![CDATA[Another NY Times expert: The fat gent heading up Yale&#039;s Obesity Center:

http://www.nytimes.com/2009/07/04/health/04patient.html

&lt;i&gt;â€œWhat matters most is your level of motivation and your willingness to change,â€ says Kelly D. Brownell, a psychologist and director of the Rudd Center for Food Policy and Obesity at Yale.&lt;/i&gt;

Really, Dr. Brownell?  How would &lt;i&gt;you&lt;/i&gt; know?]]></description>
		<content:encoded><![CDATA[<p>Another NY Times expert: The fat gent heading up Yale&#8217;s Obesity Center:</p>
<p><a href="http://www.nytimes.com/2009/07/04/health/04patient.html" rel="nofollow">http://www.nytimes.com/2009/07/04/health/04patient.html</a></p>
<p><i>â€œWhat matters most is your level of motivation and your willingness to change,â€ says Kelly D. Brownell, a psychologist and director of the Rudd Center for Food Policy and Obesity at Yale.</i></p>
<p>Really, Dr. Brownell?  How would <i>you</i> know?</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: seth</title>
		<link>http://blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/#comment-321067</link>
		<dc:creator><![CDATA[seth]]></dc:creator>
		<pubDate>Sat, 04 Jul 2009 03:36:45 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/#comment-321067</guid>
		<description><![CDATA[True, she does mention randomization. Maybe her mistake was to ask an epidemiologist about clinical trials. Not realizing that epidemiologists do surveys, not experiments. Equation of the groups being compared is a much bigger deal for epidemiologists than experimenters.

I don&#039;t think it&#039;s obvious that the beta-carotene clinical trials are more trustworthy than the other beta-carotene studies. I&#039;d have to know a lot more about the details before I&#039;d reach that conclusion. For example, large clinical trials allow vast possibilities for data entry errors, which will reduce differences between groups. I know an example where a transcription error wasn&#039;t noticed for 40 years.  Did the MRFIT clinical trial reach the right conclusion (of no effect)? It&#039;s still hard to know.

What neither Kolata nor her experts understand is that until something more accurate than &quot;randomized clinical trials&quot; comes along, we have no way of generally assessing their accuracy -- just as the problem with eyewitness testimony only became apparent when DNA testing came along.]]></description>
		<content:encoded><![CDATA[<p>True, she does mention randomization. Maybe her mistake was to ask an epidemiologist about clinical trials. Not realizing that epidemiologists do surveys, not experiments. Equation of the groups being compared is a much bigger deal for epidemiologists than experimenters.</p>
<p>I don&#8217;t think it&#8217;s obvious that the beta-carotene clinical trials are more trustworthy than the other beta-carotene studies. I&#8217;d have to know a lot more about the details before I&#8217;d reach that conclusion. For example, large clinical trials allow vast possibilities for data entry errors, which will reduce differences between groups. I know an example where a transcription error wasn&#8217;t noticed for 40 years.  Did the MRFIT clinical trial reach the right conclusion (of no effect)? It&#8217;s still hard to know.</p>
<p>What neither Kolata nor her experts understand is that until something more accurate than &#8220;randomized clinical trials&#8221; comes along, we have no way of generally assessing their accuracy &#8212; just as the problem with eyewitness testimony only became apparent when DNA testing came along.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Vince</title>
		<link>http://blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/#comment-321060</link>
		<dc:creator><![CDATA[Vince]]></dc:creator>
		<pubDate>Sat, 04 Jul 2009 03:08:25 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/#comment-321060</guid>
		<description><![CDATA[You&#039;re being unusually uncharitable in your reading here, Seth.  I don&#039;t see anything inaccurate in her article.  It&#039;s a bit imprecise or unclear in places (for instance, she shouldn&#039;t have said &quot;exactly&quot;), and it all seems pretty basic, but I don&#039;t see this deep ignorance of research design that you&#039;re reading into her article.

Her first principle is that you need to eliminate confounding variables so that you can be confident that differences are due to the factor that you&#039;re trying to study.  She describes random assignment as the standard way to do this (I&#039;m not sure why you think she doesn&#039;t understand random assignment when she discusses it right there, explaining why randomization is better than observational studies that try to statistically control for differences).  The second principle is saying (correctly) that larger studies give you a more precise estimate of the effect size.  Studies with a smaller sample size have wider confidence intervals.  A point estimate of a 20% reduction in risk may be misleading if the confidence interval is a 5%-35% reduction.  The third principle is that other evidence can continue to be relevant after you&#039;ve done a full study with random assignment.  She seems to reach the correct conclusions about the two examples that she describes, one (prayer) where she thinks you should doubt the results of the study because of other evidence and one (beta carotene) where she thinks that you should trust the results of the study despite the other evidence, although you&#039;re right that she doesn&#039;t give much of an explanation of how to reach these conclusions.]]></description>
		<content:encoded><![CDATA[<p>You&#8217;re being unusually uncharitable in your reading here, Seth.  I don&#8217;t see anything inaccurate in her article.  It&#8217;s a bit imprecise or unclear in places (for instance, she shouldn&#8217;t have said &#8220;exactly&#8221;), and it all seems pretty basic, but I don&#8217;t see this deep ignorance of research design that you&#8217;re reading into her article.</p>
<p>Her first principle is that you need to eliminate confounding variables so that you can be confident that differences are due to the factor that you&#8217;re trying to study.  She describes random assignment as the standard way to do this (I&#8217;m not sure why you think she doesn&#8217;t understand random assignment when she discusses it right there, explaining why randomization is better than observational studies that try to statistically control for differences).  The second principle is saying (correctly) that larger studies give you a more precise estimate of the effect size.  Studies with a smaller sample size have wider confidence intervals.  A point estimate of a 20% reduction in risk may be misleading if the confidence interval is a 5%-35% reduction.  The third principle is that other evidence can continue to be relevant after you&#8217;ve done a full study with random assignment.  She seems to reach the correct conclusions about the two examples that she describes, one (prayer) where she thinks you should doubt the results of the study because of other evidence and one (beta carotene) where she thinks that you should trust the results of the study despite the other evidence, although you&#8217;re right that she doesn&#8217;t give much of an explanation of how to reach these conclusions.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Andrew Gelman</title>
		<link>http://blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/#comment-321045</link>
		<dc:creator><![CDATA[Andrew Gelman]]></dc:creator>
		<pubDate>Sat, 04 Jul 2009 01:15:09 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/#comment-321045</guid>
		<description><![CDATA[Yes, it sounds like you use the terms &quot;effect&quot; and &quot;experiment&quot; in different ways than statisticians do.  Which is fine; I realize that our usages aren&#039;t always so intuitive.]]></description>
		<content:encoded><![CDATA[<p>Yes, it sounds like you use the terms &#8220;effect&#8221; and &#8220;experiment&#8221; in different ways than statisticians do.  Which is fine; I realize that our usages aren&#8217;t always so intuitive.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: seth</title>
		<link>http://blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/#comment-320976</link>
		<dc:creator><![CDATA[seth]]></dc:creator>
		<pubDate>Fri, 03 Jul 2009 22:58:42 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/#comment-320976</guid>
		<description><![CDATA[Andrew, by &quot;effect&quot; I meant experimental effect. The whole discussion is about experiments.Â  The sex ratio stuff in your Amer Scientist article isn&#039;t experimental (= does not come from experiments). I&#039;m happy to learn about an example that contradicts what I said but it would need to be an experiment.

Your sister&#039;s research isn&#039;t experimental psychology, it&#039;s developmental psychology. I agree, the term &lt;em&gt;experimental psychology&lt;/em&gt; (= perceptual and cognitive psychology and animal learning) isn&#039;t terribly clear to outsiders. Developmental psychology experiments tend to have larger n&#039;s than experimental psychology experiments.]]></description>
		<content:encoded><![CDATA[<p>Andrew, by &#8220;effect&#8221; I meant experimental effect. The whole discussion is about experiments.Â  The sex ratio stuff in your Amer Scientist article isn&#8217;t experimental (= does not come from experiments). I&#8217;m happy to learn about an example that contradicts what I said but it would need to be an experiment.</p>
<p>Your sister&#8217;s research isn&#8217;t experimental psychology, it&#8217;s developmental psychology. I agree, the term <em>experimental psychology</em> (= perceptual and cognitive psychology and animal learning) isn&#8217;t terribly clear to outsiders. Developmental psychology experiments tend to have larger n&#8217;s than experimental psychology experiments.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Andrew Gelman</title>
		<link>http://blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/#comment-320956</link>
		<dc:creator><![CDATA[Andrew Gelman]]></dc:creator>
		<pubDate>Fri, 03 Jul 2009 21:58:02 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/#comment-320956</guid>
		<description><![CDATA[Seth:

In the best of all worlds, Kolata would&#039;ve spoken with a medical-statistics expert such as Stephen Senn, John Carlin, or Chris Schmid.  But, given whom she did talk with, I&#039;m thinking it would&#039;ve helped for her to broaden her understanding by talking with some social science statisticians.

Regarding principle 2:  it depends on what you&#039;re studying.  If it&#039;s a rare condition, you might need a large sample size to get lots of cases.  Or if there&#039;s a high level of natural variability, you&#039;ll need a high sample size to see signal amid noise.  The example I was referring to was the sex ratio of babies, which is close to purely random.  As we discuss in our paper, you need a very large sample size to discover patterns here.  You might argue that a 1% change in the probability of a girl birth is so tiny that nobody should care about it--and maybe you&#039;re right--but that&#039;s the context of some things that people study.  Medical outcomes can be highly unpredictable, and small effects can be of interest to people.

Anyway, my main point is not to defend large studies but to disagree with the implications of your two statements, claiming  (1) people with statistical understanding judge &quot;the reliability of an effect&quot; by the p-value, and (2) &quot;small experiments with reliable results are more impressive than large experiments with equally reliable results.&quot;  Not so.  As discussed in our American Scientist article, statistical significance doesn&#039;t necessarily tell you much at all, if the estimate is so large as to be scientifically implausible.  That&#039;s something you can learn from statistical power analysis, or from Bayesian inference.

Finally . . . lots of psychology studies have n&gt;20.  Just for example, my sister&#039;s most cited article is based on a study with 104 kids.  If you can find it with n=10, great.  But I don&#039;t think that experimental psychologists have been barraging Susan with questions about why her sample size is so much more than 10.]]></description>
		<content:encoded><![CDATA[<p>Seth:</p>
<p>In the best of all worlds, Kolata would&#8217;ve spoken with a medical-statistics expert such as Stephen Senn, John Carlin, or Chris Schmid.  But, given whom she did talk with, I&#8217;m thinking it would&#8217;ve helped for her to broaden her understanding by talking with some social science statisticians.</p>
<p>Regarding principle 2:  it depends on what you&#8217;re studying.  If it&#8217;s a rare condition, you might need a large sample size to get lots of cases.  Or if there&#8217;s a high level of natural variability, you&#8217;ll need a high sample size to see signal amid noise.  The example I was referring to was the sex ratio of babies, which is close to purely random.  As we discuss in our paper, you need a very large sample size to discover patterns here.  You might argue that a 1% change in the probability of a girl birth is so tiny that nobody should care about it&#8211;and maybe you&#8217;re right&#8211;but that&#8217;s the context of some things that people study.  Medical outcomes can be highly unpredictable, and small effects can be of interest to people.</p>
<p>Anyway, my main point is not to defend large studies but to disagree with the implications of your two statements, claiming  (1) people with statistical understanding judge &#8220;the reliability of an effect&#8221; by the p-value, and (2) &#8220;small experiments with reliable results are more impressive than large experiments with equally reliable results.&#8221;  Not so.  As discussed in our American Scientist article, statistical significance doesn&#8217;t necessarily tell you much at all, if the estimate is so large as to be scientifically implausible.  That&#8217;s something you can learn from statistical power analysis, or from Bayesian inference.</p>
<p>Finally . . . lots of psychology studies have n&gt;20.  Just for example, my sister&#8217;s most cited article is based on a study with 104 kids.  If you can find it with n=10, great.  But I don&#8217;t think that experimental psychologists have been barraging Susan with questions about why her sample size is so much more than 10.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: seth</title>
		<link>http://blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/#comment-320910</link>
		<dc:creator><![CDATA[seth]]></dc:creator>
		<pubDate>Fri, 03 Jul 2009 19:04:14 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/#comment-320910</guid>
		<description><![CDATA[Andrew, she wanted to understand clinical trials, so she talked to some experts who do them. What she found revealed incompetence, which is interesting. Talking to quantitative political scientists or psychometricians wouldn&#039;t be a good way to learn about clinical trials.
Principle 2: An experimental psychologist who read an experiment (in psychology) with a large sample size (e.g., n = 20) would be suspicious: Why such a large sample size? It must mean the effect is weak or maybe they did the experiment with n = 10 (a typical size) and didn&#039;t find anything. Anyway, it would mean something was off.  In this sense, the larger the sample size, the less trustworthy.]]></description>
		<content:encoded><![CDATA[<p>Andrew, she wanted to understand clinical trials, so she talked to some experts who do them. What she found revealed incompetence, which is interesting. Talking to quantitative political scientists or psychometricians wouldn&#8217;t be a good way to learn about clinical trials.<br />
Principle 2: An experimental psychologist who read an experiment (in psychology) with a large sample size (e.g., n = 20) would be suspicious: Why such a large sample size? It must mean the effect is weak or maybe they did the experiment with n = 10 (a typical size) and didn&#8217;t find anything. Anyway, it would mean something was off.  In this sense, the larger the sample size, the less trustworthy.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Andrew Gelman</title>
		<link>http://blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/#comment-320908</link>
		<dc:creator><![CDATA[Andrew Gelman]]></dc:creator>
		<pubDate>Fri, 03 Jul 2009 18:30:23 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/#comment-320908</guid>
		<description><![CDATA[It sounds like this reporter talked with the wrong experts.  Not to overgeneralize, but I&#039;ve noticed that a lot of biologists have a pretty naive understanding of statistics.  It would maybe better for her to talk with some econometricians, psychometricians, or quantitative political scientists and sociologists.

That said, I think you&#039;re confused on Principle 2 above.  With a small sample size, you can still find statistically significant differences by chance--and they&#039;ll look huge!  Take a look at my paper with Weakliem that just appeared in American Scientist.]]></description>
		<content:encoded><![CDATA[<p>It sounds like this reporter talked with the wrong experts.  Not to overgeneralize, but I&#8217;ve noticed that a lot of biologists have a pretty naive understanding of statistics.  It would maybe better for her to talk with some econometricians, psychometricians, or quantitative political scientists and sociologists.</p>
<p>That said, I think you&#8217;re confused on Principle 2 above.  With a small sample size, you can still find statistically significant differences by chance&#8211;and they&#8217;ll look huge!  Take a look at my paper with Weakliem that just appeared in American Scientist.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Charles</title>
		<link>http://blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/#comment-320797</link>
		<dc:creator><![CDATA[Charles]]></dc:creator>
		<pubDate>Fri, 03 Jul 2009 06:21:30 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2009/07/02/how-much-should-we-trust-clinical-trials/#comment-320797</guid>
		<description><![CDATA[Well I don&#039;t know about clinical trials, but I know we shouldn&#039;t ever trust Kolata. She had an illuminating back-and-forth with Gary Taubes at one point (you documented it) which demonstrated her inability to read and understand simple English sentences, much less basic science.]]></description>
		<content:encoded><![CDATA[<p>Well I don&#8217;t know about clinical trials, but I know we shouldn&#8217;t ever trust Kolata. She had an illuminating back-and-forth with Gary Taubes at one point (you documented it) which demonstrated her inability to read and understand simple English sentences, much less basic science.</p>
]]></content:encoded>
	</item>
</channel>
</rss>

<!-- Performance optimized by W3 Total Cache. Learn more: http://www.w3-edge.com/wordpress-plugins/

Content Delivery Network via Amazon Web Services: S3: srblogfiles.s3.amazonaws.com

 Served from: blog.sethroberts.net @ 2014-10-12 23:19:10 by W3 Total Cache -->