<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd"
xmlns:rawvoice="http://www.rawvoice.com/rawvoiceRssModule/"

	>
<channel>
	<title>Comments on: Voodoo Correlations in Social Neuroscience</title>
	<atom:link href="http://blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/feed/" rel="self" type="application/rss+xml" />
	<link>http://blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/</link>
	<description>Personal Science, Self-Experimentation, Scientific Method</description>
	<lastBuildDate>Wed, 17 Sep 2014 20:18:18 +0000</lastBuildDate>
		<sy:updatePeriod>hourly</sy:updatePeriod>
		<sy:updateFrequency>1</sy:updateFrequency>
	<generator>http://wordpress.org/?v=3.7.1</generator>
	<item>
		<title>By: Matthew Lieberman</title>
		<link>http://blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-409113</link>
		<dc:creator><![CDATA[Matthew Lieberman]]></dc:creator>
		<pubDate>Wed, 24 Mar 2010 20:16:13 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-409113</guid>
		<description><![CDATA[For anyone interested, there was a public debate on Voodoo Correlations last fall at the Society of Experimental Social Psychologists between Piotr Winkielman (one of the authors on the Voodoo paper) and myself (Matt Lieberman).  The debate has been posted online. 

http://www.scn.ucla.edu/Voodoo&amp;TypeII.html]]></description>
		<content:encoded><![CDATA[<p>For anyone interested, there was a public debate on Voodoo Correlations last fall at the Society of Experimental Social Psychologists between Piotr Winkielman (one of the authors on the Voodoo paper) and myself (Matt Lieberman).  The debate has been posted online. </p>
<p><a href="http://www.scn.ucla.edu/Voodoo&#038;TypeII.html" rel="nofollow">http://www.scn.ucla.edu/Voodoo&#038;TypeII.html</a></p>
]]></content:encoded>
	</item>
	<item>
		<title>By: seth</title>
		<link>http://blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-377443</link>
		<dc:creator><![CDATA[seth]]></dc:creator>
		<pubDate>Fri, 01 Jan 2010 14:13:34 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-377443</guid>
		<description><![CDATA[Justin, because I&#039;m in China I can&#039;t get YouTube.]]></description>
		<content:encoded><![CDATA[<p>Justin, because I&#8217;m in China I can&#8217;t get YouTube.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Justin</title>
		<link>http://blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-377400</link>
		<dc:creator><![CDATA[Justin]]></dc:creator>
		<pubDate>Fri, 01 Jan 2010 12:14:28 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-377400</guid>
		<description><![CDATA[Hi Seth, 
Any chance of some constructive feedback on this video on the above study?
[youtube=http://www.youtube.com/watch?v=nMZvpVwfObE]
Regards

Justin]]></description>
		<content:encoded><![CDATA[<p>Hi Seth,<br />
Any chance of some constructive feedback on this video on the above study?<br />
[youtube=http://www.youtube.com/watch?v=nMZvpVwfObE]<br />
Regards</p>
<p>Justin</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: &#8216;Voodoo Correlations in Social Neuroscience&#8217; &#171; The Amazing World of Psychiatry: A Psychiatry Blog</title>
		<link>http://blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-266121</link>
		<dc:creator><![CDATA[&#8216;Voodoo Correlations in Social Neuroscience&#8217; &#171; The Amazing World of Psychiatry: A Psychiatry Blog]]></dc:creator>
		<pubDate>Sat, 31 Jan 2009 01:08:54 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-266121</guid>
		<description><![CDATA[[...] Seths Blog article [...]]]></description>
		<content:encoded><![CDATA[<p>[...] Seths Blog article [...]</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: seth</title>
		<link>http://blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-265809</link>
		<dc:creator><![CDATA[seth]]></dc:creator>
		<pubDate>Thu, 29 Jan 2009 16:56:06 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-265809</guid>
		<description><![CDATA[Thanks, Matt.]]></description>
		<content:encoded><![CDATA[<p>Thanks, Matt.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Matt</title>
		<link>http://blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-265793</link>
		<dc:creator><![CDATA[Matt]]></dc:creator>
		<pubDate>Thu, 29 Jan 2009 16:03:42 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-265793</guid>
		<description><![CDATA[So, we combine a significance threshold (i.e. p-values less than .005 or .001) with an extent threshold (i.e. there have to be at least 10 contiguous voxels that all have p-values less than the significance threshold).  This is a standard procedure used throughout cognitive neuroscience for the past 15 years.]]></description>
		<content:encoded><![CDATA[<p>So, we combine a significance threshold (i.e. p-values less than .005 or .001) with an extent threshold (i.e. there have to be at least 10 contiguous voxels that all have p-values less than the significance threshold).  This is a standard procedure used throughout cognitive neuroscience for the past 15 years.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Matt</title>
		<link>http://blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-265780</link>
		<dc:creator><![CDATA[Matt]]></dc:creator>
		<pubDate>Thu, 29 Jan 2009 15:04:59 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-265780</guid>
		<description><![CDATA[Sorry for the typos (yes its the correlations that are inflated, but that&#039;s not what the test is testing - its testing for reliable non-zero relationships).  Your Jordan analogy doesn&#039;t quite apply.  That would assume that we are making claims about the average correlation in the brain but only reporting on a subset of voxels (and pretending they are all the voxels).  We aren&#039;t making claims about how the brain as a whole or on average relates to personality - rather we are looking for which regions do correlate reliably and then providing descriptive statistics for those that do.

To get the Jordan analogy right, the question would be &quot;Are there certain days of the week when Jordan shoots a higher percentage than others?&quot;.  We&#039;d have him shoot 100 free throws each day of the week for say 10 weeks.  So we&#039;d have 1000 data points for each of the seven days of the week.  We wouldn&#039;t care at all what his average across all days was, just how each day compares to each other.  If his averages were 30% on mondays, 90% on fridays and 60% on all other days, we would say something interesting is happening on mondays and fridays, report that test and the descriptives that go along with it (e.g. 90%).  Now if we reported that Jordan shoots 90% on average because we claimed that fridays were the only days were looking at, we&#039;d be in trouble, but nobody does that.  Our question isn&#039;t the average, but rather, when is there something different from average going on.]]></description>
		<content:encoded><![CDATA[<p>Sorry for the typos (yes its the correlations that are inflated, but that&#8217;s not what the test is testing &#8211; its testing for reliable non-zero relationships).  Your Jordan analogy doesn&#8217;t quite apply.  That would assume that we are making claims about the average correlation in the brain but only reporting on a subset of voxels (and pretending they are all the voxels).  We aren&#8217;t making claims about how the brain as a whole or on average relates to personality &#8211; rather we are looking for which regions do correlate reliably and then providing descriptive statistics for those that do.</p>
<p>To get the Jordan analogy right, the question would be &#8220;Are there certain days of the week when Jordan shoots a higher percentage than others?&#8221;.  We&#8217;d have him shoot 100 free throws each day of the week for say 10 weeks.  So we&#8217;d have 1000 data points for each of the seven days of the week.  We wouldn&#8217;t care at all what his average across all days was, just how each day compares to each other.  If his averages were 30% on mondays, 90% on fridays and 60% on all other days, we would say something interesting is happening on mondays and fridays, report that test and the descriptives that go along with it (e.g. 90%).  Now if we reported that Jordan shoots 90% on average because we claimed that fridays were the only days were looking at, we&#8217;d be in trouble, but nobody does that.  Our question isn&#8217;t the average, but rather, when is there something different from average going on.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: john</title>
		<link>http://blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-265748</link>
		<dc:creator><![CDATA[john]]></dc:creator>
		<pubDate>Thu, 29 Jan 2009 12:40:50 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-265748</guid>
		<description><![CDATA[Seth,

&quot;Letâ€™s say I select a subset of free throws where Michael Jordan missed. Then I compute his free throw percentage over only those free throws. It is 0%. To report that 0% as if it means something is  . . . well, call it what you want. As far as I can tell, that is basically what you did.&quot;

I may have missed something here, but isn&#039;t that what Matt is claiming that Vul has done? Isn&#039;t one of the strong arguements in the Lieberman paper that Ed Vul simply hand picked results from papers that would show the effect he wanted to show and ignored the others that didn&#039;t? In fact when Matt puts all the data in to the analysis there is no bias in the correlation coeffecient at all.]]></description>
		<content:encoded><![CDATA[<p>Seth,</p>
<p>&#8220;Letâ€™s say I select a subset of free throws where Michael Jordan missed. Then I compute his free throw percentage over only those free throws. It is 0%. To report that 0% as if it means something is  . . . well, call it what you want. As far as I can tell, that is basically what you did.&#8221;</p>
<p>I may have missed something here, but isn&#8217;t that what Matt is claiming that Vul has done? Isn&#8217;t one of the strong arguements in the Lieberman paper that Ed Vul simply hand picked results from papers that would show the effect he wanted to show and ignored the others that didn&#8217;t? In fact when Matt puts all the data in to the analysis there is no bias in the correlation coeffecient at all.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: seth</title>
		<link>http://blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-265697</link>
		<dc:creator><![CDATA[seth]]></dc:creator>
		<pubDate>Thu, 29 Jan 2009 08:47:24 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-265697</guid>
		<description><![CDATA[Matt, could you post again the last part of your comment? It was cut off.

What you call &quot;selection bias&quot; --  computing a correlation using only voxels selected by looking at the data and thereby inflating the correlation -- doesn&#039;t inflate &quot;tests&quot; it inflates correlations.

Nor are &quot;voxels&quot; inflated (by &quot;voxel&quot; I guess you mean the correlation computed for just one voxel), it is the correlation computed over many voxels that is inflated. That&#039;s when you got into trouble -- by computing a number that might be grossly inflated.

Let&#039;s say I select a subset of free throw attempts where Michael Jordan missed. Then I compute his free throw percentage over only those free throws. It is 0%. To report that 0% as if it means something isÂ  . . . well, call it what you want. As far as I can tell, that is basically what you did.]]></description>
		<content:encoded><![CDATA[<p>Matt, could you post again the last part of your comment? It was cut off.</p>
<p>What you call &#8220;selection bias&#8221; &#8212;  computing a correlation using only voxels selected by looking at the data and thereby inflating the correlation &#8212; doesn&#8217;t inflate &#8220;tests&#8221; it inflates correlations.</p>
<p>Nor are &#8220;voxels&#8221; inflated (by &#8220;voxel&#8221; I guess you mean the correlation computed for just one voxel), it is the correlation computed over many voxels that is inflated. That&#8217;s when you got into trouble &#8212; by computing a number that might be grossly inflated.</p>
<p>Let&#8217;s say I select a subset of free throw attempts where Michael Jordan missed. Then I compute his free throw percentage over only those free throws. It is 0%. To report that 0% as if it means something isÂ  . . . well, call it what you want. As far as I can tell, that is basically what you did.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Matt</title>
		<link>http://blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-265661</link>
		<dc:creator><![CDATA[Matt]]></dc:creator>
		<pubDate>Thu, 29 Jan 2009 05:49:43 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-265661</guid>
		<description><![CDATA[Actually, what I am denying is that there was any post-hoc selection of a subset of the data.  Running multiple comparisons leads to inflated effect sizes but that has nothing to do with &quot;post-hoc selection&quot;.  And actually, the main point of their article was not that there is inflation but rather that this inflation is so great that the results should be considered worthless and likely spurious and also that the methods used to obtain the results are invalid and therefore the results themselves are invalid.  These tests are run in order to identify regions where there are reliably non-zero correlations and they are a perfectly valid way of doing so.  To report descriptive statistics is entirely valid as well.  Since we seem to be talking past each other, let&#039;s consider one last example.  Let&#039;s say I run my 40,000 independent tests on my 40,000 voxels.  You would admit at this point there has been no &quot;selection bias&quot; inflating these tests, correct?  You might have some large effects due to sampling fluxuations, but our Figure 1 shows that with normal fMRI sample sizes and appropriate correction for multiple comparisons, this is relatively rare (Vul&#039;s simulation was done assuming 10 subjects which is not representative of fMRI studies).  Let&#039;s further assume that I submit my paper to the journal with a 200 page table that lists the p-value (along with descriptive statistics) for every voxel in the brain.  Still no selection bias inflating these tests, correct?  If you sorted this table by p-value we&#039;d still be ok, right?  Now the editor comes along and says &quot;we can&#039;t have a 200 page table&quot; so cut off everything with a p-value worse than ___ and add a note to indicate that all other voxels had p-values above that threshold.  The voxels that remained would be no more inflated after this editorial decision than before - its just a matter of convention for displaying data.  This is what we all do and there is no &quot;non-independence error&quot; as Vul claims.]]></description>
		<content:encoded><![CDATA[<p>Actually, what I am denying is that there was any post-hoc selection of a subset of the data.  Running multiple comparisons leads to inflated effect sizes but that has nothing to do with &#8220;post-hoc selection&#8221;.  And actually, the main point of their article was not that there is inflation but rather that this inflation is so great that the results should be considered worthless and likely spurious and also that the methods used to obtain the results are invalid and therefore the results themselves are invalid.  These tests are run in order to identify regions where there are reliably non-zero correlations and they are a perfectly valid way of doing so.  To report descriptive statistics is entirely valid as well.  Since we seem to be talking past each other, let&#8217;s consider one last example.  Let&#8217;s say I run my 40,000 independent tests on my 40,000 voxels.  You would admit at this point there has been no &#8220;selection bias&#8221; inflating these tests, correct?  You might have some large effects due to sampling fluxuations, but our Figure 1 shows that with normal fMRI sample sizes and appropriate correction for multiple comparisons, this is relatively rare (Vul&#8217;s simulation was done assuming 10 subjects which is not representative of fMRI studies).  Let&#8217;s further assume that I submit my paper to the journal with a 200 page table that lists the p-value (along with descriptive statistics) for every voxel in the brain.  Still no selection bias inflating these tests, correct?  If you sorted this table by p-value we&#8217;d still be ok, right?  Now the editor comes along and says &#8220;we can&#8217;t have a 200 page table&#8221; so cut off everything with a p-value worse than ___ and add a note to indicate that all other voxels had p-values above that threshold.  The voxels that remained would be no more inflated after this editorial decision than before &#8211; its just a matter of convention for displaying data.  This is what we all do and there is no &#8220;non-independence error&#8221; as Vul claims.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: seth</title>
		<link>http://blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-265657</link>
		<dc:creator><![CDATA[seth]]></dc:creator>
		<pubDate>Thu, 29 Jan 2009 05:22:33 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-265657</guid>
		<description><![CDATA[Gee, Matt, you&#039;re still not denying that post-hoc selection of a subset inflates the correlations. Which -- correct me if I&#039;m wrong -- was the main point of Vul et al. Along with the point that this inflation was not made clear in the published papers.
I&#039;m still curious: What correction for multiple tests did your research group use?]]></description>
		<content:encoded><![CDATA[<p>Gee, Matt, you&#8217;re still not denying that post-hoc selection of a subset inflates the correlations. Which &#8212; correct me if I&#8217;m wrong &#8212; was the main point of Vul et al. Along with the point that this inflation was not made clear in the published papers.<br />
I&#8217;m still curious: What correction for multiple tests did your research group use?</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Matt</title>
		<link>http://blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-265648</link>
		<dc:creator><![CDATA[Matt]]></dc:creator>
		<pubDate>Thu, 29 Jan 2009 04:53:44 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-265648</guid>
		<description><![CDATA[A single test is run on each voxel in the brain (not on a subset).  Let&#039;s say that&#039;s 40,000 tests.  Each of those tests are computed independently.  We get a p-value for each.  That is the only inferential step that occurs.  Two things then follow this.  First, we have to report something in the journal manuscript itself.  Convention for all of cognitive neuroscience (as well as most behavioral studies as well) is that tests that meet a significance threshold are reported and all other tests are assumed to have been computed but had p-values that did not meet the significance threshold (i.e. readers then know which clusters have p-values below the threshold and which have p-values above the threshold).  For the tests that are reported, because their p-values were below some conventional level, you then have to decide what to report to describe the data.  You might report means and standard deviations, you might report t or Z statistics, or you might report r or d.  In each case, these are descriptive statistics rather than an additional inferential biased because it uses the same criteria again.  

Let&#039;s say we have 4 classrooms and you run t-tests comparing the heights of the students in each two classroom combination.  Imagine that you find that classrooms 2 &amp; 4 show a reliable difference form each other on height.  You might then say &quot;Of the combinations tested there was a significant difference between classrooms 2 &amp; 4&quot; and readers would assume there were no other significant differences.  There&#039;s no selection bias here - just reporting of what&#039;s significant.  Now are you allowed to tell readers what the average heights are in classrooms 2 &amp; 4?  Because Vul et al., say you can&#039;t, but of course you can and should.  If you don&#039;t believe me, read Vul&#039;s other chapter with Kanwisher where he says that if you run a whole-brain contrast comparing responses to emotional faces to neutral faces, that you can&#039;t graph the results from significant clusters.

Incidentally &quot;corrections&quot; was plural because there are multiple techniques that can be used for correcting.  In any particular case, a research group does a single form of correction.]]></description>
		<content:encoded><![CDATA[<p>A single test is run on each voxel in the brain (not on a subset).  Let&#8217;s say that&#8217;s 40,000 tests.  Each of those tests are computed independently.  We get a p-value for each.  That is the only inferential step that occurs.  Two things then follow this.  First, we have to report something in the journal manuscript itself.  Convention for all of cognitive neuroscience (as well as most behavioral studies as well) is that tests that meet a significance threshold are reported and all other tests are assumed to have been computed but had p-values that did not meet the significance threshold (i.e. readers then know which clusters have p-values below the threshold and which have p-values above the threshold).  For the tests that are reported, because their p-values were below some conventional level, you then have to decide what to report to describe the data.  You might report means and standard deviations, you might report t or Z statistics, or you might report r or d.  In each case, these are descriptive statistics rather than an additional inferential biased because it uses the same criteria again.  </p>
<p>Let&#8217;s say we have 4 classrooms and you run t-tests comparing the heights of the students in each two classroom combination.  Imagine that you find that classrooms 2 &amp; 4 show a reliable difference form each other on height.  You might then say &#8220;Of the combinations tested there was a significant difference between classrooms 2 &amp; 4&#8243; and readers would assume there were no other significant differences.  There&#8217;s no selection bias here &#8211; just reporting of what&#8217;s significant.  Now are you allowed to tell readers what the average heights are in classrooms 2 &amp; 4?  Because Vul et al., say you can&#8217;t, but of course you can and should.  If you don&#8217;t believe me, read Vul&#8217;s other chapter with Kanwisher where he says that if you run a whole-brain contrast comparing responses to emotional faces to neutral faces, that you can&#8217;t graph the results from significant clusters.</p>
<p>Incidentally &#8220;corrections&#8221; was plural because there are multiple techniques that can be used for correcting.  In any particular case, a research group does a single form of correction.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: seth</title>
		<link>http://blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-265638</link>
		<dc:creator><![CDATA[seth]]></dc:creator>
		<pubDate>Thu, 29 Jan 2009 04:03:18 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-265638</guid>
		<description><![CDATA[Matt, you write &quot;there is no &#039;second&#039; analysis&quot;. I don&#039;t follow. You use the word &quot;subset&quot;. It sounds like one analysis is done on all the data and then a second analysis is done on a &quot;subset&quot; of the data. Why this second analysis somehow doesn&#039;t count as &quot;second&quot; isn&#039;t clear. Sure, the first and second analyses aren&#039;t the same but that isn&#039;t the point. The point is that the selection of the subset increases the size of the reported correlation. Since you don&#039;t seem to dispute that I am not entirely clear what your point is. 

Vul et al go on and on about inflated -- spuriously large, impossibly large -- correlations. Not two inferential tests. And the procedure you have described does exactly that: inflates correlations.

&quot;standard corrections for multiple tests are implemented&quot; -- you make it sound so easy. Yet nobody does multiple corrections so why you write &quot;corrections&quot; instead of &quot;correction&quot; isn&#039;t clear. Which &quot;standard correction&quot; did you use?]]></description>
		<content:encoded><![CDATA[<p>Matt, you write &#8220;there is no &#8216;second&#8217; analysis&#8221;. I don&#8217;t follow. You use the word &#8220;subset&#8221;. It sounds like one analysis is done on all the data and then a second analysis is done on a &#8220;subset&#8221; of the data. Why this second analysis somehow doesn&#8217;t count as &#8220;second&#8221; isn&#8217;t clear. Sure, the first and second analyses aren&#8217;t the same but that isn&#8217;t the point. The point is that the selection of the subset increases the size of the reported correlation. Since you don&#8217;t seem to dispute that I am not entirely clear what your point is. </p>
<p>Vul et al go on and on about inflated &#8212; spuriously large, impossibly large &#8212; correlations. Not two inferential tests. And the procedure you have described does exactly that: inflates correlations.</p>
<p>&#8220;standard corrections for multiple tests are implemented&#8221; &#8212; you make it sound so easy. Yet nobody does multiple corrections so why you write &#8220;corrections&#8221; instead of &#8220;correction&#8221; isn&#8217;t clear. Which &#8220;standard correction&#8221; did you use?</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Matt</title>
		<link>http://blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-265611</link>
		<dc:creator><![CDATA[Matt]]></dc:creator>
		<pubDate>Thu, 29 Jan 2009 02:11:56 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-265611</guid>
		<description><![CDATA[If you read p. 3 in the paragraph that the footnote comes from, you&#039;ll see how everyone contacted runs their regressions.  &quot;When a whole-brain regression analysis is conducted, the goal is typically to identify regions of the brain whose activity shows a reliable non-zero correlation with another individual difference variable.  A likelihood estimate that this correlation was produced in the absence of any true effect (e.g. a p-value) is computed for every voxel in the brain without any selection of voxels to test.  This is the only inferential step in the procedure, and standard corrections for multiple tests are implemented to avoid false positive results.  Subsequently, descriptive statistics (e.g. effect sizes) are reported on a subset of voxels or clusters.  The descriptive statistics reported are not an additional inferential step, so there is no â€œsecond analysis.â€  For any particular sample size, the t and r-values are merely re-descriptions of the p-values obtained in the one inferential step and provide no additional inferential information of their own.&quot;  Each of the authors indicated they did this rather than the two inferential steps that Vul et al. describe.  Vul et al. were under the mistaken impression that people were using the personality variable to choose voxels to test and then subsequently running a new inferential test on those selected voxels.  Not true.  We run the test on all voxels and then report those that are reliable along with descriptive statistics.  Everyone knows that all the other brain regions were also tested but did not pass the threshold for significance.  

How could they have gotten this so wrong in their paper?  Vul et al. asked multiple choice questions that did not allow researchers to explain what they were doing and because Vul et al. never told researchers why they were asking the questions, there was no way for the researchers to guess Vul et al&#039;s true purpose and realize that Vul et al&#039;s questions could not provide Vul with the answers they needed.  They also never followed up with any of the authors to ask if they were properly characterizing the conducted research.]]></description>
		<content:encoded><![CDATA[<p>If you read p. 3 in the paragraph that the footnote comes from, you&#8217;ll see how everyone contacted runs their regressions.  &#8220;When a whole-brain regression analysis is conducted, the goal is typically to identify regions of the brain whose activity shows a reliable non-zero correlation with another individual difference variable.  A likelihood estimate that this correlation was produced in the absence of any true effect (e.g. a p-value) is computed for every voxel in the brain without any selection of voxels to test.  This is the only inferential step in the procedure, and standard corrections for multiple tests are implemented to avoid false positive results.  Subsequently, descriptive statistics (e.g. effect sizes) are reported on a subset of voxels or clusters.  The descriptive statistics reported are not an additional inferential step, so there is no â€œsecond analysis.â€  For any particular sample size, the t and r-values are merely re-descriptions of the p-values obtained in the one inferential step and provide no additional inferential information of their own.&#8221;  Each of the authors indicated they did this rather than the two inferential steps that Vul et al. describe.  Vul et al. were under the mistaken impression that people were using the personality variable to choose voxels to test and then subsequently running a new inferential test on those selected voxels.  Not true.  We run the test on all voxels and then report those that are reliable along with descriptive statistics.  Everyone knows that all the other brain regions were also tested but did not pass the threshold for significance.  </p>
<p>How could they have gotten this so wrong in their paper?  Vul et al. asked multiple choice questions that did not allow researchers to explain what they were doing and because Vul et al. never told researchers why they were asking the questions, there was no way for the researchers to guess Vul et al&#8217;s true purpose and realize that Vul et al&#8217;s questions could not provide Vul with the answers they needed.  They also never followed up with any of the authors to ask if they were properly characterizing the conducted research.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: seth</title>
		<link>http://blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-265416</link>
		<dc:creator><![CDATA[seth]]></dc:creator>
		<pubDate>Wed, 28 Jan 2009 17:05:54 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-265416</guid>
		<description><![CDATA[Bill, I found the Lieberman et al. reply too vague to be persuasive. It claims that some papers were falsely accused (they are accused of doing X but they did not do X) but no specific examples are given. Care to give an example of a falsely-accused paper?]]></description>
		<content:encoded><![CDATA[<p>Bill, I found the Lieberman et al. reply too vague to be persuasive. It claims that some papers were falsely accused (they are accused of doing X but they did not do X) but no specific examples are given. Care to give an example of a falsely-accused paper?</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Bill</title>
		<link>http://blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-265384</link>
		<dc:creator><![CDATA[Bill]]></dc:creator>
		<pubDate>Wed, 28 Jan 2009 15:18:53 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-265384</guid>
		<description><![CDATA[I commend Vul and colleagues for raising awareness of statistical issues in fMRI analysis.  However, as several strong rebuttals have begun to emerge and the merits of individual papers are being explored more closely, it is rapidly becoming clear that the Vul critique may have incorrectly targeted a good number of papers that conducted legitimate and valid analyses.   The Vul paper seems to have totally ignored the a priori hypotheses and theoretical basis of the selected regions that drove the analysis of most of the papers they criticize.  Further, as was well articulated in the recent response by Lieberman and colleagues (http://www.scn.ucla.edu/pdf/LiebermanBerkmanWager(invitedreply).pdf ), the Vul critique seems to have misconstrued the way most of the analyses were actually conducted.  In other words, they appear to be accusing the authors of doing something they actually did not do.  This is unfortunate.]]></description>
		<content:encoded><![CDATA[<p>I commend Vul and colleagues for raising awareness of statistical issues in fMRI analysis.  However, as several strong rebuttals have begun to emerge and the merits of individual papers are being explored more closely, it is rapidly becoming clear that the Vul critique may have incorrectly targeted a good number of papers that conducted legitimate and valid analyses.   The Vul paper seems to have totally ignored the a priori hypotheses and theoretical basis of the selected regions that drove the analysis of most of the papers they criticize.  Further, as was well articulated in the recent response by Lieberman and colleagues (<a href="http://www.scn.ucla.edu/pdf/LiebermanBerkmanWager(invitedreply)" rel="nofollow">http://www.scn.ucla.edu/pdf/LiebermanBerkmanWager(invitedreply)</a>.pdf ), the Vul critique seems to have misconstrued the way most of the analyses were actually conducted.  In other words, they appear to be accusing the authors of doing something they actually did not do.  This is unfortunate.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Matthew Lieberman</title>
		<link>http://blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-264744</link>
		<dc:creator><![CDATA[Matthew Lieberman]]></dc:creator>
		<pubDate>Tue, 27 Jan 2009 19:51:39 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-264744</guid>
		<description><![CDATA[Our invited reply

&lt;a rel=&quot;nofollow&quot; href=&quot;http://www.blog.sethroberts.net/&quot; rel=&quot;nofollow&quot;&gt;http://www.scn.ucla.edu/pdf/LiebermanBerkmanWager(invitedreply).pdf &lt;/a&gt;]]></description>
		<content:encoded><![CDATA[<p>Our invited reply</p>
<p><a rel="nofollow" href="http://www.blog.sethroberts.net/" rel="nofollow"></a><a href="http://www.scn.ucla.edu/pdf/LiebermanBerkmanWager(invitedreply)" rel="nofollow">http://www.scn.ucla.edu/pdf/LiebermanBerkmanWager(invitedreply)</a>.pdf </p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Ed Vul</title>
		<link>http://blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-259506</link>
		<dc:creator><![CDATA[Ed Vul]]></dc:creator>
		<pubDate>Thu, 15 Jan 2009 15:32:57 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-259506</guid>
		<description><![CDATA[For those interested, you can find our response to this reply here:
http://edvul.com/voodoorebuttal.php  
Cheers, Ed.]]></description>
		<content:encoded><![CDATA[<p>For those interested, you can find our response to this reply here:<br />
<a href="http://edvul.com/voodoorebuttal.php" rel="nofollow">http://edvul.com/voodoorebuttal.php</a><br />
Cheers, Ed.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Christian Keysers</title>
		<link>http://blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-258657</link>
		<dc:creator><![CDATA[Christian Keysers]]></dc:creator>
		<pubDate>Tue, 13 Jan 2009 07:46:50 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-258657</guid>
		<description><![CDATA[See www.bcn-nic.nl/replyVul.pdf for a reply from some of the authors]]></description>
		<content:encoded><![CDATA[<p>See <a href="http://www.bcn-nic.nl/replyVul.pdf" rel="nofollow">http://www.bcn-nic.nl/replyVul.pdf</a> for a reply from some of the authors</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Geoff</title>
		<link>http://blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-258078</link>
		<dc:creator><![CDATA[Geoff]]></dc:creator>
		<pubDate>Sun, 11 Jan 2009 19:57:48 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2008/12/28/voodoo-correlations-in-social-neuroscience/#comment-258078</guid>
		<description><![CDATA[I love this quote by James &quot;Another case of donâ€™t believe everything you read, even (especially) if it comes from a well-respected source.&quot; when he is doing exactly that when reading n abstarct from one paper. Ridiculous!]]></description>
		<content:encoded><![CDATA[<p>I love this quote by James &#8220;Another case of donâ€™t believe everything you read, even (especially) if it comes from a well-respected source.&#8221; when he is doing exactly that when reading n abstarct from one paper. Ridiculous!</p>
]]></content:encoded>
	</item>
</channel>
</rss>

<!-- Performance optimized by W3 Total Cache. Learn more: http://www.w3-edge.com/wordpress-plugins/

Content Delivery Network via Amazon Web Services: S3: srblogfiles.s3.amazonaws.com

 Served from: blog.sethroberts.net @ 2014-10-12 23:42:32 by W3 Total Cache -->