<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd"
xmlns:rawvoice="http://www.rawvoice.com/rawvoiceRssModule/"

	>
<channel>
	<title>Comments on: The Buttermind Experiment</title>
	<atom:link href="http://blog.sethroberts.net/2011/01/29/the-buttermind-experiment/feed/" rel="self" type="application/rss+xml" />
	<link>http://blog.sethroberts.net/2011/01/29/the-buttermind-experiment/</link>
	<description>Personal Science, Self-Experimentation, Scientific Method</description>
	<lastBuildDate>Wed, 17 Sep 2014 20:18:18 +0000</lastBuildDate>
		<sy:updatePeriod>hourly</sy:updatePeriod>
		<sy:updateFrequency>1</sy:updateFrequency>
	<generator>http://wordpress.org/?v=3.7.1</generator>
	<item>
		<title>By: Phil Goetz</title>
		<link>http://blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-938912</link>
		<dc:creator><![CDATA[Phil Goetz]]></dc:creator>
		<pubDate>Mon, 12 Dec 2011 21:39:55 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-938912</guid>
		<description><![CDATA[The results may be significant, but it looks like the initial difference between the groups is even more significant.  Why?]]></description>
		<content:encoded><![CDATA[<p>The results may be significant, but it looks like the initial difference between the groups is even more significant.  Why?</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Jazi yechezkel zilber</title>
		<link>http://blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-935696</link>
		<dc:creator><![CDATA[Jazi yechezkel zilber]]></dc:creator>
		<pubDate>Tue, 29 Nov 2011 09:40:06 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-935696</guid>
		<description><![CDATA[Seth,

I see that with that much participants we got a significant but not earth shaking p-value.  While in your own data, you usually show very strng p-values.

Why is there such relative weakness in the data here of other people relative to you?


Do you think you have more stability in other changes so that there is less noise in your data? Better calibrations? Whatever?
Training effect?

Or because this group just had a very short time and three conditions? 


I am wondering because I am contemplating to group people for self experimentation, and these data are somewhat of an indication that maybe it is harder to get significant results than i initially thought]]></description>
		<content:encoded><![CDATA[<p>Seth,</p>
<p>I see that with that much participants we got a significant but not earth shaking p-value.  While in your own data, you usually show very strng p-values.</p>
<p>Why is there such relative weakness in the data here of other people relative to you?</p>
<p>Do you think you have more stability in other changes so that there is less noise in your data? Better calibrations? Whatever?<br />
Training effect?</p>
<p>Or because this group just had a very short time and three conditions? </p>
<p>I am wondering because I am contemplating to group people for self experimentation, and these data are somewhat of an indication that maybe it is harder to get significant results than i initially thought</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Alex Chernavsky</title>
		<link>http://blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-847659</link>
		<dc:creator><![CDATA[Alex Chernavsky]]></dc:creator>
		<pubDate>Tue, 08 Feb 2011 04:23:59 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-847659</guid>
		<description><![CDATA[Seth, yes, but if you wanted to know the value of the absolute improvement in reaction time, you&#039;d have to know whether the tool was giving you results that were true.  Also -- and I&#039;m far from an expert on statistics -- if it turned out that the reaction-time application gave noisy readings that fluctuated +/- 20% from the true value, would this fact not make it more difficult to separate the signal from the noise in these types of &quot;Buttermind&quot; experiments?

In any case, perhaps I&#039;m being irrational, but when I weigh myself every morning, I like to know that I&#039;m tracking not only changes in weight but also the real weight itself.  That&#039;s why I always check my bathroom scale against the higher-end scales in two different doctors&#039; offices, whenever I have an appointment.]]></description>
		<content:encoded><![CDATA[<p>Seth, yes, but if you wanted to know the value of the absolute improvement in reaction time, you&#8217;d have to know whether the tool was giving you results that were true.  Also &#8212; and I&#8217;m far from an expert on statistics &#8212; if it turned out that the reaction-time application gave noisy readings that fluctuated +/- 20% from the true value, would this fact not make it more difficult to separate the signal from the noise in these types of &#8220;Buttermind&#8221; experiments?</p>
<p>In any case, perhaps I&#8217;m being irrational, but when I weigh myself every morning, I like to know that I&#8217;m tracking not only changes in weight but also the real weight itself.  That&#8217;s why I always check my bathroom scale against the higher-end scales in two different doctors&#8217; offices, whenever I have an appointment.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Seth Roberts</title>
		<link>http://blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-845281</link>
		<dc:creator><![CDATA[Seth Roberts]]></dc:creator>
		<pubDate>Mon, 07 Feb 2011 08:37:47 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-845281</guid>
		<description><![CDATA[Alex, there is error in all measurements. It would be a staggering coincidence if the error in my measurements changed at exactly the time I changed to eating lots of butter. Likewise, it would be a staggering coincidence if the error in the Genomera measurements strongly correlated with the butter/no butter treatment. Try randomly sorting the butter/no butter subjects into two groups and see how often you observe a difference as large as the difference actually observed. This is the purpose of p values -- to estimate the chances of the difference you observed being due to something other than what you varied.]]></description>
		<content:encoded><![CDATA[<p>Alex, there is error in all measurements. It would be a staggering coincidence if the error in my measurements changed at exactly the time I changed to eating lots of butter. Likewise, it would be a staggering coincidence if the error in the Genomera measurements strongly correlated with the butter/no butter treatment. Try randomly sorting the butter/no butter subjects into two groups and see how often you observe a difference as large as the difference actually observed. This is the purpose of p values &#8212; to estimate the chances of the difference you observed being due to something other than what you varied.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Alex Chernavsky</title>
		<link>http://blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-844563</link>
		<dc:creator><![CDATA[Alex Chernavsky]]></dc:creator>
		<pubDate>Mon, 07 Feb 2011 05:07:07 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-844563</guid>
		<description><![CDATA[I&#039;d like to see some kind of testing done to verify that these reaction-time applications (Seth&#039;s R code and Genomera&#039;s web-based app) give accurate and precise measurements that are independent of hardware, operating system, network traffic, and other processes running on the machine.  Not sure exactly how to do this -- maybe by writing code that is able to simulate pressing keystrokes?  Or, perhaps less practically, some kind of hardware device that is able to press keys?

In any case, because the interpretation hinges on fairly small changes in reaction time, I think it makes sense to invest some time to ensure the validity of the data-gathering tools.]]></description>
		<content:encoded><![CDATA[<p>I&#8217;d like to see some kind of testing done to verify that these reaction-time applications (Seth&#8217;s R code and Genomera&#8217;s web-based app) give accurate and precise measurements that are independent of hardware, operating system, network traffic, and other processes running on the machine.  Not sure exactly how to do this &#8212; maybe by writing code that is able to simulate pressing keystrokes?  Or, perhaps less practically, some kind of hardware device that is able to press keys?</p>
<p>In any case, because the interpretation hinges on fairly small changes in reaction time, I think it makes sense to invest some time to ensure the validity of the data-gathering tools.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: EdwinBoyette</title>
		<link>http://blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-844529</link>
		<dc:creator><![CDATA[EdwinBoyette]]></dc:creator>
		<pubDate>Mon, 07 Feb 2011 04:56:10 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-844529</guid>
		<description><![CDATA[Seth, I&#039;m now two weeks into fish oil supplementation, and a week into adding sardines and butter (1/2 to 1 stick a day).  Today during my workout I noticed the weights moved faster than last workout even though I increased the weight since last time.  

I&#039;m 38 years old, 175 pounds and 6&#039;1. Today&#039;s workout was all conventional deadlifts without a belt or straps:

135 x 10 
135 x 10 
205 x 5

Work sets:
275x 4 x 4 (73ish% of  1 Repitition Max)
315 x 1 x 2 (86ish % of 1RM)
345 x 1 x 2 (94ish % of 1RM)

I&#039;m not taking any anabolic or androgenic compounds.  Since some of the higher weights seems to be dependent on the CNS, I wonder if I have gotten a bit of a boost from the additional oils in the diet. My pace was very fast tonight with short rest times between sets so I was really suprised how quickly the bar moved tonight.]]></description>
		<content:encoded><![CDATA[<p>Seth, I&#8217;m now two weeks into fish oil supplementation, and a week into adding sardines and butter (1/2 to 1 stick a day).  Today during my workout I noticed the weights moved faster than last workout even though I increased the weight since last time.  </p>
<p>I&#8217;m 38 years old, 175 pounds and 6&#8217;1. Today&#8217;s workout was all conventional deadlifts without a belt or straps:</p>
<p>135 x 10<br />
135 x 10<br />
205 x 5</p>
<p>Work sets:<br />
275x 4 x 4 (73ish% of  1 Repitition Max)<br />
315 x 1 x 2 (86ish % of 1RM)<br />
345 x 1 x 2 (94ish % of 1RM)</p>
<p>I&#8217;m not taking any anabolic or androgenic compounds.  Since some of the higher weights seems to be dependent on the CNS, I wonder if I have gotten a bit of a boost from the additional oils in the diet. My pace was very fast tonight with short rest times between sets so I was really suprised how quickly the bar moved tonight.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Seth Roberts</title>
		<link>http://blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-844291</link>
		<dc:creator><![CDATA[Seth Roberts]]></dc:creator>
		<pubDate>Mon, 07 Feb 2011 03:01:00 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-844291</guid>
		<description><![CDATA[If you would like to learn more about data transformation, here&#039;s a paper by me about it:

http://www.sethroberts.net/articles/2008%20Transform%20your%20data.pdf

Failing to transform your data is often like throwing away a large fraction of it.]]></description>
		<content:encoded><![CDATA[<p>If you would like to learn more about data transformation, here&#8217;s a paper by me about it:</p>
<p><a href="http://www.sethroberts.net/articles/2008%20Transform%20your%20data.pdf" rel="nofollow">http://www.sethroberts.net/articles/2008%20Transform%20your%20data.pdf</a></p>
<p>Failing to transform your data is often like throwing away a large fraction of it.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Seth Roberts</title>
		<link>http://blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-841997</link>
		<dc:creator><![CDATA[Seth Roberts]]></dc:creator>
		<pubDate>Sun, 06 Feb 2011 10:46:19 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-841997</guid>
		<description><![CDATA[Celeste, how does your fat intake affect your mental clarity?

Chris Hogg, I hope you will tell me what your biostatistician friend says about data transformation. It isn&#039;t controversial; you can see for example Exploratory Data Analysis by John Tukey for a discussion of the reasons for transformation. However, many biostatisticians seem to be living in a statistical dark age, the most obvious signs of which are they don&#039;t plot data, they don&#039;t transform data, and they do too many tests.]]></description>
		<content:encoded><![CDATA[<p>Celeste, how does your fat intake affect your mental clarity?</p>
<p>Chris Hogg, I hope you will tell me what your biostatistician friend says about data transformation. It isn&#8217;t controversial; you can see for example Exploratory Data Analysis by John Tukey for a discussion of the reasons for transformation. However, many biostatisticians seem to be living in a statistical dark age, the most obvious signs of which are they don&#8217;t plot data, they don&#8217;t transform data, and they do too many tests.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Chris Hogg</title>
		<link>http://blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-840993</link>
		<dc:creator><![CDATA[Chris Hogg]]></dc:creator>
		<pubDate>Sun, 06 Feb 2011 03:23:51 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-840993</guid>
		<description><![CDATA[I really enjoyed reading this discussion.  I think discussions like these are helpful, and will push us citizen scientists to create well designed studies and generate meaningful results.  I definitely enjoyed all of the really good technical discussion about the instrument, potential bias due to latency, internet connection speed, etc.  It is interesting how we all have such different perspectives, as i would not have immediately thought of this as an issue. 

On the issue of data integrity and validity of results, i think a lot of things that happened in this study are pretty common in any clinical study: significant data outliers, potential technical errors in data capture, etc.  In any study (even $100M pharma-sponsored trials) you will have these issues. Even simple blood tests get messed up, have variability in the measurement, etc.  This is just part of the intrinsic variability you will see in any study, and you will try to compensate for it by studying a large enough population.  I agree with Seth and Eri that this does not harm the study at all.   As long as you assume that these &#039;errors&#039; are equally balanced between the arms (meaning there is no reason why Butter should have more errors than Coconut oil), this should not adversely impact, or skew, the result.  The worst thing you could do here is start to throw out &#039;outliers&#039; because you think there must be something wrong with their results.  Then you have really biased the data and damaged the study.

There are a couple of ways to proactively try to address some of the challenges with these potential &#039;errors&#039;.  Most commonly, is simply randomization.  When a subject enters the study you assign them to Arm X or Arm Y of the study.  You pre-specify things you think could influence the result (age, gender, other baseline characteristics, co-morbid diseases, fast typing in this case, etc) and you make sure all of these parameters are equally balanced between the arms.  The other is to set up a priori some data analysis rules (ie. you could specify before the study that results 3 standard deviations away from the mean are &#039;bad&#039; and exclude them from the analysis).  This poses all sorts of problems usually, and will open the study to controversy.

Eri, one thing that would be really interesting in a next wave is to see if you could employ some form of randomization.  one thing that strikes me from the study that i would like to fix is the different average baseline values for the 3 arms.  In this case, i agree with Seth that Butter started out with the least room to improve, yet improved the most.  If Coconut Oil had improved the most, i would not have believed the result and assumed it was regression to the mean.  I wonder if you could tell people what intervention to use after they did a run-in test without butter or coconut oil.  then you could &#039;balance&#039; the coconut oil and butter arms so that they started from the same average baseline number.  this would make the result a lot easier to interpret, and would just be cool if you could pull it off.  just for another random thought you could also have them do a typing speed test if you think this is a source of bias, and try to randomize on that as well.

The only real question i had is on using a Log transform of the data to do the analysis.  i know this is common in other fields (mathematics, physics, etc), i am just not sure if this is commonly done in biostatistics and analysis of clinical data.  i will ask a biostats friend of mine if this is a common practice or is specifically not done for some particular reason.

Thanks again for a good discussion.]]></description>
		<content:encoded><![CDATA[<p>I really enjoyed reading this discussion.  I think discussions like these are helpful, and will push us citizen scientists to create well designed studies and generate meaningful results.  I definitely enjoyed all of the really good technical discussion about the instrument, potential bias due to latency, internet connection speed, etc.  It is interesting how we all have such different perspectives, as i would not have immediately thought of this as an issue. </p>
<p>On the issue of data integrity and validity of results, i think a lot of things that happened in this study are pretty common in any clinical study: significant data outliers, potential technical errors in data capture, etc.  In any study (even $100M pharma-sponsored trials) you will have these issues. Even simple blood tests get messed up, have variability in the measurement, etc.  This is just part of the intrinsic variability you will see in any study, and you will try to compensate for it by studying a large enough population.  I agree with Seth and Eri that this does not harm the study at all.   As long as you assume that these &#8216;errors&#8217; are equally balanced between the arms (meaning there is no reason why Butter should have more errors than Coconut oil), this should not adversely impact, or skew, the result.  The worst thing you could do here is start to throw out &#8216;outliers&#8217; because you think there must be something wrong with their results.  Then you have really biased the data and damaged the study.</p>
<p>There are a couple of ways to proactively try to address some of the challenges with these potential &#8216;errors&#8217;.  Most commonly, is simply randomization.  When a subject enters the study you assign them to Arm X or Arm Y of the study.  You pre-specify things you think could influence the result (age, gender, other baseline characteristics, co-morbid diseases, fast typing in this case, etc) and you make sure all of these parameters are equally balanced between the arms.  The other is to set up a priori some data analysis rules (ie. you could specify before the study that results 3 standard deviations away from the mean are &#8216;bad&#8217; and exclude them from the analysis).  This poses all sorts of problems usually, and will open the study to controversy.</p>
<p>Eri, one thing that would be really interesting in a next wave is to see if you could employ some form of randomization.  one thing that strikes me from the study that i would like to fix is the different average baseline values for the 3 arms.  In this case, i agree with Seth that Butter started out with the least room to improve, yet improved the most.  If Coconut Oil had improved the most, i would not have believed the result and assumed it was regression to the mean.  I wonder if you could tell people what intervention to use after they did a run-in test without butter or coconut oil.  then you could &#8216;balance&#8217; the coconut oil and butter arms so that they started from the same average baseline number.  this would make the result a lot easier to interpret, and would just be cool if you could pull it off.  just for another random thought you could also have them do a typing speed test if you think this is a source of bias, and try to randomize on that as well.</p>
<p>The only real question i had is on using a Log transform of the data to do the analysis.  i know this is common in other fields (mathematics, physics, etc), i am just not sure if this is commonly done in biostatistics and analysis of clinical data.  i will ask a biostats friend of mine if this is a common practice or is specifically not done for some particular reason.</p>
<p>Thanks again for a good discussion.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Celeste</title>
		<link>http://blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-840455</link>
		<dc:creator><![CDATA[Celeste]]></dc:creator>
		<pubDate>Sun, 06 Feb 2011 00:15:19 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-840455</guid>
		<description><![CDATA[Hi Seth,
I never met you, but I&#039;m Mel&#039;s sister. I followed a link to your blog from an email she sent me. This is very funny that you did an experiment like this. I actually do a lot of experimenting myself with nutrition since I was a kid...although never a &quot;real&quot; one like this. 

Raw grassfed butter or sometimes other animal fats has been a big part of my diet for years (10). I sometimes go through 2-3 lbs a week. I have noticed clarity of mind to be a factor of my fat intake. I also found other neurological symptoms like anxiety, neuralgia, MS like symptoms, that have come and gone depending on the source of fat I am using. Fun to look through your blog.]]></description>
		<content:encoded><![CDATA[<p>Hi Seth,<br />
I never met you, but I&#8217;m Mel&#8217;s sister. I followed a link to your blog from an email she sent me. This is very funny that you did an experiment like this. I actually do a lot of experimenting myself with nutrition since I was a kid&#8230;although never a &#8220;real&#8221; one like this. </p>
<p>Raw grassfed butter or sometimes other animal fats has been a big part of my diet for years (10). I sometimes go through 2-3 lbs a week. I have noticed clarity of mind to be a factor of my fat intake. I also found other neurological symptoms like anxiety, neuralgia, MS like symptoms, that have come and gone depending on the source of fat I am using. Fun to look through your blog.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Eri Gentry</title>
		<link>http://blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-839567</link>
		<dc:creator><![CDATA[Eri Gentry]]></dc:creator>
		<pubDate>Sat, 05 Feb 2011 17:58:06 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-839567</guid>
		<description><![CDATA[@Kirk, I should be clearer about the large values. The one I mentioned above was due to a &quot;bug,&quot; if you will, in the system: when two keys were hit at once, no answer was submitted. However, two numbers would remain in the answer field - which was ALWAYS a wrong answer in our test - so, the one taking the test would have to delete both numbers and re-answer.

Other large values could be due to things you suggested: various distractions or thoughtful hesitation. 

A few more things of note about the test design and how data was collected:

- Only correct answers were submitted. Reaction times were collected for all correct answers and the test continued until the user achieved 32 correct answers.  There is technically no penalty for wrong answers (data is thrown out); however some users mentioned this had the effect of &quot;psyching them out,&quot; bringing their confidence levels down for the remainder of the test. I believe this effect decreases over time, though I have nothing to back it up but observations of myself. 
- Questions were designed to have only single-digit answers [0-9], and
- Questions automatically advanced after a single keystroke, whether number, letter, or symbol. Any incorrect stroke was called wrong and tossed out. The only bug I saw here was the one I mentioned - when, if two keys are hit simultaneously, the question does not advance (even though the answer is clearly wrong), but must be corrected by manual deletion then entry of a single item.

Kirk, I appreciate the time you&#039;ve spent thinking about the data. Certainly, there is room for improvement in group experiment study design, and even in building the tools to complement different studies. My hope is that more people like you will engage to help create resources on both sides. Generally, being at the beginning of an age that has &quot;room for improvement,&quot; AKA one the will keep getting better (with this one, perhaps, the participatory health age... unless some better name sticks :P) is a really exciting place to be.  

I&#039;ll keep doing studies like this and plan to directly integrate user feedback as I move forward. So, everyone, please keep ideas flowing, and thanks for sharing! 

Lastly, re: the gunfight... hehe. Thanks, I&#039;m strangely flattered :)]]></description>
		<content:encoded><![CDATA[<p>@Kirk, I should be clearer about the large values. The one I mentioned above was due to a &#8220;bug,&#8221; if you will, in the system: when two keys were hit at once, no answer was submitted. However, two numbers would remain in the answer field &#8211; which was ALWAYS a wrong answer in our test &#8211; so, the one taking the test would have to delete both numbers and re-answer.</p>
<p>Other large values could be due to things you suggested: various distractions or thoughtful hesitation. </p>
<p>A few more things of note about the test design and how data was collected:</p>
<p>- Only correct answers were submitted. Reaction times were collected for all correct answers and the test continued until the user achieved 32 correct answers.  There is technically no penalty for wrong answers (data is thrown out); however some users mentioned this had the effect of &#8220;psyching them out,&#8221; bringing their confidence levels down for the remainder of the test. I believe this effect decreases over time, though I have nothing to back it up but observations of myself.<br />
- Questions were designed to have only single-digit answers [0-9], and<br />
- Questions automatically advanced after a single keystroke, whether number, letter, or symbol. Any incorrect stroke was called wrong and tossed out. The only bug I saw here was the one I mentioned &#8211; when, if two keys are hit simultaneously, the question does not advance (even though the answer is clearly wrong), but must be corrected by manual deletion then entry of a single item.</p>
<p>Kirk, I appreciate the time you&#8217;ve spent thinking about the data. Certainly, there is room for improvement in group experiment study design, and even in building the tools to complement different studies. My hope is that more people like you will engage to help create resources on both sides. Generally, being at the beginning of an age that has &#8220;room for improvement,&#8221; AKA one the will keep getting better (with this one, perhaps, the participatory health age&#8230; unless some better name sticks <img src='http://srblogfiles.s3.amazonaws.com/wp-includes/images/smilies/icon_razz.gif' alt=':P' class='wp-smiley' /> ) is a really exciting place to be.  </p>
<p>I&#8217;ll keep doing studies like this and plan to directly integrate user feedback as I move forward. So, everyone, please keep ideas flowing, and thanks for sharing! </p>
<p>Lastly, re: the gunfight&#8230; hehe. Thanks, I&#8217;m strangely flattered <img src='http://srblogfiles.s3.amazonaws.com/wp-includes/images/smilies/icon_smile.gif' alt=':)' class='wp-smiley' /> </p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Kirk</title>
		<link>http://blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-832551</link>
		<dc:creator><![CDATA[Kirk]]></dc:creator>
		<pubDate>Thu, 03 Feb 2011 04:19:33 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-832551</guid>
		<description><![CDATA[@Eri, thank you for the explanation of the large value.  My conclusion is that the larger values, say, any of those which are double the value of the mean for that participant, are probably the result of a miscalculation (or miskeying) which resulted in a correction.  

The explanation of the use of the Javascript package seems appropriate.  It is not a language I know so I can&#039;t review it, but since Javascript was chosen, it proves to me that good design minds understood how to solve this kind of problem space, and thus network latency is probably not an issue.

I agree the data shows that people improve at this task over time.  I find myself reluctant to draw further conclusions, given the limited amount of data, the continued improvement with practice, and the high penalty for mistakes.  Would the results be different if those who made the most mistakes have been assigned to another group?  Yet here I must bow out, having exhausted my meager capabilities at data analysis.  

One final learning: never challenge Eri to a quick-draw gunfight.]]></description>
		<content:encoded><![CDATA[<p>@Eri, thank you for the explanation of the large value.  My conclusion is that the larger values, say, any of those which are double the value of the mean for that participant, are probably the result of a miscalculation (or miskeying) which resulted in a correction.  </p>
<p>The explanation of the use of the Javascript package seems appropriate.  It is not a language I know so I can&#8217;t review it, but since Javascript was chosen, it proves to me that good design minds understood how to solve this kind of problem space, and thus network latency is probably not an issue.</p>
<p>I agree the data shows that people improve at this task over time.  I find myself reluctant to draw further conclusions, given the limited amount of data, the continued improvement with practice, and the high penalty for mistakes.  Would the results be different if those who made the most mistakes have been assigned to another group?  Yet here I must bow out, having exhausted my meager capabilities at data analysis.  </p>
<p>One final learning: never challenge Eri to a quick-draw gunfight.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Eri Gentry</title>
		<link>http://blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-829276</link>
		<dc:creator><![CDATA[Eri Gentry]]></dc:creator>
		<pubDate>Wed, 02 Feb 2011 06:12:28 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-829276</guid>
		<description><![CDATA[@Kirk, I have no certain technical or metaphysical explanation for why my response times were any different. However, I&#039;ve been typing for years and worked in my parents&#039; grocery store as a kid, where I added prices together in my head every day. I&#039;m sure both of those skills together gave me a slightly natural advantage. During the study, I simply tried to respond correctly as quickly as possible. 

For all but one of the testing days, I was using a laptop tethered to the internet via an android phone, as I had neither wireless nor hard-wired internet.  So, I was by no means connected locally.

I remember the &#039;burp.&#039; I typed the wrong answer, deleted, and retyped the right answer. I remember because that aggravated me. =/ Btw, great eye to attention for catching that!!!

Kirk, if you haven&#039;t already, I definitely recommend trying the math test for yourself (link above). The experience will likely inform some of your previous questions.

@Alex, below is a response by our lead developer to a question similar to yours. Hope it helps!

&#039;&#039;&#039;
I can&#039;t speak to Seth&#039;s R program.  For ours, I altered the jQuizMe plug-in by adding responseStartTime and responseEndTime variables.  I bookended them on the tightest path I could find within the plugin.  Certainly there is a bit of intervening plug-in plumbing, but I don&#039;t believe it&#039;s adding any sort of interesting lag.  (The modified JS is at http://s3.amazonaws.com/genomera/instruments/math/scripts/jquery.quiz.js  if anyone wants to have a read through particularly poorly crafted javascript. You can search for anything I added by &quot;[jtz]&quot;.)

If we feel like there&#039;s a problem, I can construct a static HTML test harness that measures millisecond response time within the context of a single function.  This would get the jquery plug-in stuff out of the way.  We can each take it and see if there is some kind of meaningful change.

At the very least, I&#039;m going to state that our instrument is consistent.  So, even if it&#039;s &#039;slow&#039;, it&#039;s not going to skew the data. 
&#039;&#039;&#039;

I want to point you all to our feedback page at http://getsatisfaction.com/genomera
We have used this as place to host questions and answers about Butter Mind and the math test used for it.]]></description>
		<content:encoded><![CDATA[<p>@Kirk, I have no certain technical or metaphysical explanation for why my response times were any different. However, I&#8217;ve been typing for years and worked in my parents&#8217; grocery store as a kid, where I added prices together in my head every day. I&#8217;m sure both of those skills together gave me a slightly natural advantage. During the study, I simply tried to respond correctly as quickly as possible. </p>
<p>For all but one of the testing days, I was using a laptop tethered to the internet via an android phone, as I had neither wireless nor hard-wired internet.  So, I was by no means connected locally.</p>
<p>I remember the &#8216;burp.&#8217; I typed the wrong answer, deleted, and retyped the right answer. I remember because that aggravated me. =/ Btw, great eye to attention for catching that!!!</p>
<p>Kirk, if you haven&#8217;t already, I definitely recommend trying the math test for yourself (link above). The experience will likely inform some of your previous questions.</p>
<p>@Alex, below is a response by our lead developer to a question similar to yours. Hope it helps!</p>
<p>&#8221;&#8217;<br />
I can&#8217;t speak to Seth&#8217;s R program.  For ours, I altered the jQuizMe plug-in by adding responseStartTime and responseEndTime variables.  I bookended them on the tightest path I could find within the plugin.  Certainly there is a bit of intervening plug-in plumbing, but I don&#8217;t believe it&#8217;s adding any sort of interesting lag.  (The modified JS is at <a href="http://s3.amazonaws.com/genomera/instruments/math/scripts/jquery.quiz.js" rel="nofollow">http://s3.amazonaws.com/genomera/instruments/math/scripts/jquery.quiz.js</a>  if anyone wants to have a read through particularly poorly crafted javascript. You can search for anything I added by &#8220;[jtz]&#8220;.)</p>
<p>If we feel like there&#8217;s a problem, I can construct a static HTML test harness that measures millisecond response time within the context of a single function.  This would get the jquery plug-in stuff out of the way.  We can each take it and see if there is some kind of meaningful change.</p>
<p>At the very least, I&#8217;m going to state that our instrument is consistent.  So, even if it&#8217;s &#8216;slow&#8217;, it&#8217;s not going to skew the data.<br />
&#8221;&#8217;</p>
<p>I want to point you all to our feedback page at <a href="http://getsatisfaction.com/genomera" rel="nofollow">http://getsatisfaction.com/genomera</a><br />
We have used this as place to host questions and answers about Butter Mind and the math test used for it.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Alex Chernavsky</title>
		<link>http://blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-828754</link>
		<dc:creator><![CDATA[Alex Chernavsky]]></dc:creator>
		<pubDate>Wed, 02 Feb 2011 02:34:40 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-828754</guid>
		<description><![CDATA[I have a question for Eri (and/or Greg Biggers) about how the reaction-time application actually works.  Is the timing done locally (i.e., on the user&#039;s computer) with the results simply reported back to the server -- or is the timing done on the server?  And if it&#039;s done locally, is the timing independent of the hardware and independent of other processes that might be running on the user&#039;s machine?]]></description>
		<content:encoded><![CDATA[<p>I have a question for Eri (and/or Greg Biggers) about how the reaction-time application actually works.  Is the timing done locally (i.e., on the user&#8217;s computer) with the results simply reported back to the server &#8212; or is the timing done on the server?  And if it&#8217;s done locally, is the timing independent of the hardware and independent of other processes that might be running on the user&#8217;s machine?</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Seth Roberts</title>
		<link>http://blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-828749</link>
		<dc:creator><![CDATA[Seth Roberts]]></dc:creator>
		<pubDate>Wed, 02 Feb 2011 02:32:27 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-828749</guid>
		<description><![CDATA[Alex, I used a log transform on the data -- that is, analyzed log(x) rather than x -- because the data were far more symmetric on a log scale than on the original scale. Here is a paper by me about transforming data:

http://www.sethroberts.net/articles/2008%20Transform%20your%20data.pdf

if you want to read more about data transformation, see Exploratory Data Analysis by John Tukey.]]></description>
		<content:encoded><![CDATA[<p>Alex, I used a log transform on the data &#8212; that is, analyzed log(x) rather than x &#8212; because the data were far more symmetric on a log scale than on the original scale. Here is a paper by me about transforming data:</p>
<p><a href="http://www.sethroberts.net/articles/2008%20Transform%20your%20data.pdf" rel="nofollow">http://www.sethroberts.net/articles/2008%20Transform%20your%20data.pdf</a></p>
<p>if you want to read more about data transformation, see Exploratory Data Analysis by John Tukey.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Kirk</title>
		<link>http://blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-828487</link>
		<dc:creator><![CDATA[Kirk]]></dc:creator>
		<pubDate>Wed, 02 Feb 2011 01:20:10 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-828487</guid>
		<description><![CDATA[Thank you for the explanation, Eri, yes, I was confused about the specifics.  

Eri, do you have an explanation for why your data looks extraordinarily different compared to that of other participants?  Most people seemed to struggle to get into the 900s, yet your data mostly sits in the 700s and 600s, and you even generated some astonishing low values (in the 200s and 300s).  There was only one major &#039;burp&#039;, a value of 1464 (when the other row values averaged in the 700s).  

My speculation is that you tested on a local system connected via a local network, just as Seth tests on a local system (his laptop).  

A second question would be about the design itself: does each simple math question require a request/response from the Genomera server to the client, or were all the questions packaged together in one blob, such as a lengthy Javascript, which was shipped to the client, where the questions were answered locally, and the entire summary shipped back to the server?]]></description>
		<content:encoded><![CDATA[<p>Thank you for the explanation, Eri, yes, I was confused about the specifics.  </p>
<p>Eri, do you have an explanation for why your data looks extraordinarily different compared to that of other participants?  Most people seemed to struggle to get into the 900s, yet your data mostly sits in the 700s and 600s, and you even generated some astonishing low values (in the 200s and 300s).  There was only one major &#8216;burp&#8217;, a value of 1464 (when the other row values averaged in the 700s).  </p>
<p>My speculation is that you tested on a local system connected via a local network, just as Seth tests on a local system (his laptop).  </p>
<p>A second question would be about the design itself: does each simple math question require a request/response from the Genomera server to the client, or were all the questions packaged together in one blob, such as a lengthy Javascript, which was shipped to the client, where the questions were answered locally, and the entire summary shipped back to the server?</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Eri Gentry</title>
		<link>http://blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-827689</link>
		<dc:creator><![CDATA[Eri Gentry]]></dc:creator>
		<pubDate>Tue, 01 Feb 2011 20:59:28 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-827689</guid>
		<description><![CDATA[Perhaps not everyone is on the same page as to what is represented in the spreadsheet. It wasn&#039;t clear to me at first sight.

Each row contains a single participant&#039;s (1) name, (2) time the test was begun, (3) reaction times in milliseconds for each correct answer (total=32), and (4) average of that test&#039;s reaction time in milliseconds

We typically see greater reaction times for the first question &quot;Q01,&quot; which I believe is due to adjustment of starting a test. In my own experience taking the test, I was able to mentally prepare myself for the test&#039;s start after practice. This was a little like psyching myself up before some sort of physical exercise. 

Again, in my own experience, my reaction times varied between questions because some answers came to me more quickly than others. This is fairly common. Some people are brilliant at addition and multiplication, but not so much at division. I usually took a moment longer to answer division questions than addition or subtraction.  And, when the questions shift from, say addition (1+7=?) to division (6/2=?), it can increase the time it takes to answer because we mentally shift gears.]]></description>
		<content:encoded><![CDATA[<p>Perhaps not everyone is on the same page as to what is represented in the spreadsheet. It wasn&#8217;t clear to me at first sight.</p>
<p>Each row contains a single participant&#8217;s (1) name, (2) time the test was begun, (3) reaction times in milliseconds for each correct answer (total=32), and (4) average of that test&#8217;s reaction time in milliseconds</p>
<p>We typically see greater reaction times for the first question &#8220;Q01,&#8221; which I believe is due to adjustment of starting a test. In my own experience taking the test, I was able to mentally prepare myself for the test&#8217;s start after practice. This was a little like psyching myself up before some sort of physical exercise. </p>
<p>Again, in my own experience, my reaction times varied between questions because some answers came to me more quickly than others. This is fairly common. Some people are brilliant at addition and multiplication, but not so much at division. I usually took a moment longer to answer division questions than addition or subtraction.  And, when the questions shift from, say addition (1+7=?) to division (6/2=?), it can increase the time it takes to answer because we mentally shift gears.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Alex Chernavsky</title>
		<link>http://blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-826011</link>
		<dc:creator><![CDATA[Alex Chernavsky]]></dc:creator>
		<pubDate>Tue, 01 Feb 2011 12:50:30 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-826011</guid>
		<description><![CDATA[Seth wrote: &quot;I computed means of logs. That is, I computed an average for each subject myself after log transforming the data.&quot;

Seth, for those of us who don&#039;t have a background in statistics, can you elaborate a bit on why you analyzed the data in this way?]]></description>
		<content:encoded><![CDATA[<p>Seth wrote: &#8220;I computed means of logs. That is, I computed an average for each subject myself after log transforming the data.&#8221;</p>
<p>Seth, for those of us who don&#8217;t have a background in statistics, can you elaborate a bit on why you analyzed the data in this way?</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Seth Roberts</title>
		<link>http://blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-825715</link>
		<dc:creator><![CDATA[Seth Roberts]]></dc:creator>
		<pubDate>Tue, 01 Feb 2011 08:57:43 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-825715</guid>
		<description><![CDATA[Kirk, the data do not look odd when converted to logarithms, in the sense that the data are unimodal, roughly symmetric, and not heavy-tailed. This is common -- lots of data make more sense when converted to logarithms.

I don&#039;t find myself taking 3 times as long as normal but I am very well practiced. Practice reduces variability on a log scale, I suspect.

If there were computer bottlenecks now and then that makes finding a significant difference more impressive.

Vince, thanks for the suggestion. It is the sort of thing that it makes sense to examine in an exploratory analysis. To improve future confirmatory analyses.]]></description>
		<content:encoded><![CDATA[<p>Kirk, the data do not look odd when converted to logarithms, in the sense that the data are unimodal, roughly symmetric, and not heavy-tailed. This is common &#8212; lots of data make more sense when converted to logarithms.</p>
<p>I don&#8217;t find myself taking 3 times as long as normal but I am very well practiced. Practice reduces variability on a log scale, I suspect.</p>
<p>If there were computer bottlenecks now and then that makes finding a significant difference more impressive.</p>
<p>Vince, thanks for the suggestion. It is the sort of thing that it makes sense to examine in an exploratory analysis. To improve future confirmatory analyses.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Vince</title>
		<link>http://blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-825415</link>
		<dc:creator><![CDATA[Vince]]></dc:creator>
		<pubDate>Tue, 01 Feb 2011 03:44:26 +0000</pubDate>
		<guid isPermaLink="false">http://www.blog.sethroberts.net/2011/01/29/the-buttermind-experiment/#comment-825415</guid>
		<description><![CDATA[You should exclude the first trial for each test, since the timer for the first trial starts as soon as they click &quot;Go&quot;, which is often before they are ready to type in their responses.  Responses were a quarter of a second slower on trial 1 than on trial 2, on average, and twice as likely to be over 2 seconds (20% vs. 10%).]]></description>
		<content:encoded><![CDATA[<p>You should exclude the first trial for each test, since the timer for the first trial starts as soon as they click &#8220;Go&#8221;, which is often before they are ready to type in their responses.  Responses were a quarter of a second slower on trial 1 than on trial 2, on average, and twice as likely to be over 2 seconds (20% vs. 10%).</p>
]]></content:encoded>
	</item>
</channel>
</rss>

<!-- Performance optimized by W3 Total Cache. Learn more: http://www.w3-edge.com/wordpress-plugins/

Content Delivery Network via Amazon Web Services: S3: srblogfiles.s3.amazonaws.com

 Served from: blog.sethroberts.net @ 2014-10-12 23:04:56 by W3 Total Cache -->